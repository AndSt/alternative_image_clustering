import numpy as np
from sklearn.metrics import adjusted_mutual_info_score, normalized_mutual_info_score

from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform

import warnings
from typing import Optional
from collections import Counter
from ClusterEnsembles.ClusterEnsembles import (
    hbgf,
    mcla,
    hgpa,
    cspa,
    nmf,
    calc_objective,
)

from sklearn.cluster import KMeans

from alternative_image_clustering.data.dataset import Dataset

from alternative_image_clustering.clustering import run_single_kmeans
from alternative_image_clustering.metrics import get_multiple_labeling_metrics


def cluster_ensembles(
    base_clusters: np.ndarray,
    nclass: Optional[int] = None,
    solver: str = "hbgf",
    random_state: Optional[int] = None,
    verbose: bool = False,
) -> np.ndarray:
    """Generate a single consensus cluster using base clusters obtained from multiple clustering algorithms

    Parameters
    ----------
    base_clusters: labels generated by base clustering algorithms
    nclass: number of classes
    solver: cluster ensembles solver to use
    random_state: used for 'hgpa', 'mcla', and 'nmf'. Pass a nonnegative integer for reproducible results.
    verbose: whether to be verbose

    Return
    -------
    celabel: consensus clustering label
    """
    if nclass is None:
        nclass = -1
        for bc in base_clusters:
            len_unique_bc = len(np.unique(bc[~np.isnan(bc)]))
            nclass = max(nclass, len_unique_bc)

    if verbose:
        print("Cluster Ensembles")
        print("    - number of classes:", nclass)
        print("    - solver:", solver)
        print("    - length of base clustering labels:", base_clusters.shape[1])
        print("    - number of base clusters:", base_clusters.shape[0])

    if not (isinstance(nclass, int) and nclass > 0):
        raise ValueError(
            "Number of class must be a positive integer; got (nclass={})".format(nclass)
        )

    if not ((random_state is None) or isinstance(random_state, int)):
        raise ValueError(
            "Number of random_state must be a nonnegative integer; got (random_state={})".format(
                random_state
            )
        )

    if isinstance(random_state, int):
        random_state = abs(random_state)

    if solver == "cspa":
        if base_clusters.shape[1] > 5000:
            warnings.warn(
                "`base_clusters.shape[1]` is too large, so the use of another solvers is recommended."
            )
        celabel = cspa(base_clusters, nclass)
    elif solver == "hgpa":
        celabel = hgpa(base_clusters, nclass, random_state)
    elif solver == "mcla":
        celabel = mcla(base_clusters, nclass, random_state)
    elif solver == "hbgf":
        celabel = hbgf(base_clusters, nclass)
    elif solver == "nmf":
        celabel = nmf(base_clusters, nclass, random_state)
    elif solver == "all":
        if verbose:
            print("    - ANMI:")
        ce_solvers = {"mcla": mcla, "hbgf": hbgf}
        if base_clusters.shape[1] <= 5000:
            ce_solvers["cspa"] = cspa
            ce_solvers["nmf"] = nmf
        best_objv = None
        for name, ce_solver in ce_solvers.items():
            if ce_solver == cspa or ce_solver == hbgf:
                label = ce_solver(base_clusters, nclass)
            else:
                label = ce_solver(base_clusters, nclass, random_state)
            objv = calc_objective(base_clusters, label)
            if verbose:
                print("        -", name, ":", objv)
            if best_objv is None:
                best_objv = objv
                best_solver = name
                celabel = label
            if best_objv < objv:
                best_objv = objv
                best_solver = name
                celabel = label
        if verbose:
            print("    - Best solver:", best_solver)
    else:
        raise ValueError(
            "Invalid solver parameter: got '{}' instead of one of ('cspa', 'hgpa', 'mcla', 'hbgf', 'nmf', 'all')".format(
                solver
            )
        )

    return celabel


def label_matrix_to_linkage_and_distance_clustering(
    cluster_preds, threshold: float = 0.5, similarity_metric="ami"
):
    if not isinstance(cluster_preds, np.ndarray):
        cluster_preds = np.array(cluster_preds)

    if similarity_metric == "ami":
        sim_fct = adjusted_mutual_info_score
    elif similarity_metric == "nmi":
        sim_fct = normalized_mutual_info_score

    similarities = np.ones((cluster_preds.shape[0], cluster_preds.shape[0]))
    for i in range(len(cluster_preds) - 1):
        for j in range(i, len(cluster_preds)):
            similarities[i, j] = similarities[j, i] = (
                sim_fct(cluster_preds[i], cluster_preds[j])
                + sim_fct(cluster_preds[j], cluster_preds[i])
            ) / 2

    dists = 1 - similarities
    np.fill_diagonal(dists, 0)
    Z = linkage(squareform(dists), "ward", "cosine")
    distance_clustering = fcluster(Z, threshold, "distance")

    return Z, distance_clustering, dists


def distance_clustering_to_splitting(distance_clustering):
    clust_ids, counts = np.unique(distance_clustering, return_counts=True)

    splitting = []
    for cid, count in zip(clust_ids, counts):
        if count < 2:
            continue
        indexes = np.arange(distance_clustering.shape[0])[distance_clustering == cid]
        splitting.append(indexes)
    return splitting


def get_spltting(
    per_prompt_preds,
    threshhold_strategy: str = "min",
    n_categories: int = 2,
    similarity_metric: str = "ami",
):
    num_seps = 35
    th_values = [i / num_seps for i in range(num_seps)]

    if threshhold_strategy == "min":
        th_values = th_values
    elif threshhold_strategy == "max":
        th_values = th_values[-1::-1]
    else:
        raise ValueError("Invalid threshhold_strategy")

    for th in th_values:
        Z, distance_clustering, dists = label_matrix_to_linkage_and_distance_clustering(
            cluster_preds=per_prompt_preds,
            threshold=th,
            similarity_metric=similarity_metric,
        )
        splitting = distance_clustering_to_splitting(distance_clustering)

        if len(splitting) == n_categories:
            return Z, splitting, distance_clustering, th, dists

    raise ValueError("No splitting found")


def get_consensus_prediction(
    dataset, splitting, predictions, random_state: int = 42, solver="all"
):
    if not isinstance(predictions, np.ndarray):
        predictions = np.array(predictions)

    pred_labels = []

    for split in splitting:
        cat_labels = predictions[split]
        labels_ce = cluster_ensembles(
            cat_labels, solver=solver, random_state=random_state
        )
        pred_labels.append(labels_ce)
    return pred_labels


def get_selected_prompts_prediction(
    dataset, splitting, predictions, random_state: int = 42
):
    if not isinstance(predictions, np.ndarray):
        predictions = np.array(predictions)

    pred_labels = []
    for split in splitting:
        dataset.set_indices(split.tolist())
        embeddings = dataset.get_embeddings()

        n_clusters = [len(set(predictions[i])) for i in range(len(predictions))]
        n_clusters = Counter(n_clusters).most_common(1)[0][0]

        kmeans_results = run_single_kmeans(
            data=embeddings, n_clusters=n_clusters, random_state=random_state
        )
        pred_labels.append(kmeans_results["labels"])

    return pred_labels


def run_tgaicc(
    dataset_name: str,
    base_dir: str,
    embedding_type: str = "tfidf",
    aggregation_strategy: str = "consensus",
    threshhold_strategy: str = "min",
    similarity_metric="ami",
    random_state: int = 42,
):
    per_prompt_preds = []
    all_names = []

    if embedding_type in ["tfidf", "sbert_concat"]:

        # load data
        dataset = Dataset(
            base_dir=base_dir,
            dataset_name=dataset_name,
            embedding_type=embedding_type,
        )

        embeddings = dataset.get_embeddings()

        print("Start initial")
        for category in dataset.get_categories():
            category_ids = dataset.get_category_ids(category)

            labels = dataset.get_clustering_labels(category)  # [0:400]
            n_clusters = len(set(labels))

            # img_kmeans_results = kmeans(image_embeddings, labels, n_clusters=n_clusters)
            # per_prompt_preds.append(img_kmeans_results["best_labels"])
            # all_names.append(f"prompt_{category}_image")

            for category_id in category_ids:
                dataset.set_indices(category_id)
                embeddings = dataset.get_embeddings()  # [0:400]
                kmeans_result = run_single_kmeans(
                    data=embeddings, n_clusters=n_clusters, random_state=random_state
                )

                per_prompt_preds.append(kmeans_result["labels"])
                all_names.append({"category": category, "prompt_id": category_id})

    if embedding_type == "both":
        if aggregation_strategy == "selection":
            raise ValueError(
                "Selection aggregation strategy is not supported for both embedding type."
            )
        tfdataset = Dataset(
            base_dir=base_dir,
            dataset_name=dataset_name,
            embedding_type="tfidf",
        )
        sbdataset = Dataset(
            base_dir=base_dir,
            dataset_name=dataset_name,
            embedding_type="sbert_concat",
        )

        print("Start initial")
        for category in tfdataset.get_categories():
            category_ids = tfdataset.get_category_ids(category)
            labels = tfdataset.get_clustering_labels(category)
            n_clusters = len(set(labels))

            for category_id in category_ids:
                tfdataset.set_indices(category_id)
                tfembeddings = tfdataset.get_embeddings()

                kmeans_result = run_single_kmeans(
                    data=tfembeddings, n_clusters=n_clusters, random_state=random_state
                )

                per_prompt_preds.append(kmeans_result["labels"])
                all_names.append(
                    {"category": category, "prompt_id": f"tf_{category_id}"}
                )

        for category in sbdataset.get_categories():
            category_ids = sbdataset.get_category_ids(category)
            labels = sbdataset.get_clustering_labels(category)
            n_clusters = len(set(labels))

            for category_id in category_ids:
                sbdataset.set_indices(category_id)
                sbembeddings = sbdataset.get_embeddings()

                kmeans_result = run_single_kmeans(
                    data=sbembeddings, n_clusters=n_clusters, random_state=random_state
                )

                per_prompt_preds.append(kmeans_result["labels"])
                all_names.append(
                    {"category": category, "prompt_id": f"sb_{category_id}"}
                )

    # load data
    dataset = Dataset(
        base_dir=base_dir,
        dataset_name=dataset_name,
        embedding_type=embedding_type,
    )

    Z, splitting, distance_clustering, threshhold, dists = get_spltting(
        per_prompt_preds=per_prompt_preds,
        threshhold_strategy=threshhold_strategy,
        n_categories=len(dataset.get_categories()),
        similarity_metric=similarity_metric,
    )

    print("Adaption")
    if aggregation_strategy == "consensus":
        pred_labels = get_consensus_prediction(
            dataset=dataset,
            splitting=splitting,
            predictions=per_prompt_preds,
            random_state=random_state,
        )
    elif aggregation_strategy == "selection":
        pred_labels = get_selected_prompts_prediction(
            dataset, splitting, per_prompt_preds, random_state=random_state
        )
    else:
        raise ValueError("Invalid aggregation strategy.")

    results = {
        "aggregation_strategy": aggregation_strategy,
        "threshold": threshhold,
        "distance_clustering": distance_clustering,
        "Z_matrix": Z,
        "similarity_metric": similarity_metric,
        "pred_labels": pred_labels,
        "metrics": get_multiple_labeling_metrics(
            labels_true=dataset.get_full_clustering_labels(),
            labels_pred=np.array(pred_labels).T,
            categories=dataset.get_categories(),
        ),
        "splitting": splitting,
        "names": all_names,
    }

    return results
